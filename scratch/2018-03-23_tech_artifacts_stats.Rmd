---
title: "Technical Artifacts Stats"
author: "Nate Olson"
date: '`r Sys.Date()`'
output:
  bookdown::html_document2: 
    toc: FALSE
---



```{r techArtSetup, warning=FALSE, message=FALSE, echo = FALSE}
library(ProjectTemplate)
load.project()
library(ggfortify)
## load required libraries
```

## Seq Run Effect 
Testing for differences in mean PCR pairwise distance between sequencing runs by pipeline and diversity metric. 
```{r}
qual_df <- seq_qual_comparisons %>% 
    mutate(read_dir = paste0(read_dir, "_qual")) %>% 
    select(-sd_quality, -cov_quality) %>% 
    spread(read_dir, mean_quality) %>% 
    separate(replicate, c("biosample_id","t_fctr","seq_run_id"), remove = FALSE,extra = "merge")

tech_art_df <- seq_char_comparisons %>% 
    select(replicate,  pipe, normalization, metric, cov_num_reads, cov_total_abu) %>% 
    left_join(qual_df) %>% 
    mutate(f_type = if_else(metric %in% c("unifrac","wunifrac"), 
                            "Phylogenetic","Taxonomic"),
           weight = if_else(metric %in% c("unifrac","jaccard"),
                            "Unweighted", "Weighted")) 

tech_info_df <- tech_art_df %>% 
    filter(normalization == "RAW") %>% 
    select(biosample_id, t_fctr, seq_run_id, pipe, cov_total_abu, R1_qual, R2_qual) %>% 
    distinct() 


norm_impact <- tech_art_df %>% 
    select(biosample_id, t_fctr, seq_run_id, pipe, normalization, metric, mean_dist) %>% 
    spread(normalization, mean_dist) %>% 
    gather(normalization, mean_dist, -biosample_id, -t_fctr, -seq_run_id, -pipe, -metric, -RAW) %>% 
    mutate(dist_change = mean_dist - RAW) %>% 
    filter(!is.na(dist_change)) %>% 
    group_by(metric, pipe, normalization, seq_run_id) %>% 
    summarise(dist_change = median(dist_change)) 
```
### Fitting linear model

```{r}
tech_art_fit <- tech_art_df %>% filter(normalization == "RAW") %>% 
    rename(dist_method = metric) %>% 
    group_by(dist_method, pipe) %>% 
    nest() %>% 
    mutate(fit = map(data, ~lm(mean_dist ~ 0 + seq_run_id, data = .))) 

tech_art_anova <- tech_art_fit %>% 
    mutate(anova_fit = map(fit, anova), 
           tidy_anova = map(anova_fit, broom::tidy)) %>% 
    unnest(tidy_anova) 

tech_art_tukey <- tech_art_fit %>% 
    mutate(anova_fit = map(fit, aov), 
           tukey_fit = map(anova_fit, TukeyHSD),
           tidy_tukey = map(tukey_fit, broom::tidy)) %>% 
    unnest(tidy_tukey) 
```


### Model Results
Seq runs are significantly different for all pipelines/ metrics 
```{r}
tech_art_anova 
```

```{r}
tech_art_tukey %>% 
    mutate(adj.p.value = adj.p.value * 132) %>% 
    mutate(adj.p.value = if_else(adj.p.value > 0.1, 0.1, adj.p.value)) %>%
    mutate(pos_neg = if_else(estimate > 0, "red", "white")) %>% 
    mutate(comparison = str_remove_all(comparison, "ist_run|hu_run")) %>% 
    mutate(estimate = if_else(adj.p.value < 0.05, estimate, NaN)) %>% 
    ggplot() + 
    geom_raster(aes(x = comparison, y = pipe, fill = adj.p.value)) + 
    geom_text(aes(x = comparison, y = pipe, label = round(estimate,2), color = pos_neg)) + 
    facet_grid(dist_method~.) + 
    theme_bw() + 
    theme(axis.text.x = element_text(angle = -45, hjust = 0))
```



__NOTES__  
_Methods_  
* Fit independent linear models to d~seq_run for each pipeline and metric.  
* Used TukeyHSD to test for pairwise distances between runs.  

_Results_  
* Across pipelines NIST 1 and NIST 2 greater PCR replicate difference  
    - excluding closed ref and mothur for Jaccard and mothur for UniFrac (unweighted)  
* JHU 1 greater difference for QIIME pipelines, exluding Deblur for weighted metrics  
    - Unweighted UniFrac only closed ref  
    - Jaccard not de-novo  
* JHU1 lower PCR difference for data and mothur for NIST 1 and 2 for all metrics  
* JHU 1 significantly lower than JHU 2  for mothur (Jaccard and Unifrac), dada2 UniFrac  

_Interpretation_   
* QIIME pipelines less robust to lower seq quality   
* Expected lower diff for NIST 2 compared to JHU2 but consistent results with NIST 1, not sure why. Potentially due to higher R2 seq quality...   
    * Unaccounted for lab effect????  
* Deblur failed for JHU1   
* Lack of difference in pcr dist for JHU1 and JHU2 with mothur and dada, similar repeat between low and high quality runs????  

_Potential analysis improvements_   
* Mixed effects model using raw pairwise distances and replicate as random effect   


### Raw Stats
```{r}
tech_art_fit %>% mutate(aov_fit = map(fit, aov), 
                   tidy_aov = map(aov_fit, broom::tidy)) %>% 
    unnest(tidy_aov) %>% 
    mutate(sumsq = round(sumsq, 3),
           meansq = round(meansq, 3),
           statistic = round(statistic, 1)) %>% 
    knitr::kable()
```

### Fit Summary Statistics 
```{r}
tech_art_fit %>% mutate(glance_fit = map(fit, broom::glance)) %>% 
    unnest(glance_fit) %>% 
    select(-data, -fit) %>% 
    gather(metric, value, -dist_method, -pipe) %>% 
    spread(dist_method, value) %>% 
    knitr::kable(digits = 3)
```

### Diagnostic Plots
```{r}
tech_art_fit %>% 
    mutate(diag_plots = map2(fit, dist_method, ~{autoplot(.x) + ggtitle(.y)})) %>% 
    {print(.$diag_plots)}
```


## Normalization Comparison 
Evaluating the impact of different normalization methods no repeatability for the different pipelines and diversity metrics.  
```{r}
norm_level_ord <- c("RAW","rare2000", "rare5000", "rare10000", "rareq15",
                    "CSS", "RLE", "TMM", "TSS", "UQ")
tech_art_norm_fit <- tech_art_df %>% 
    ungroup() %>% 
    mutate(norm = factor(normalization, levels = norm_level_ord)) %>%
    filter(seq_run_id %in% c("nist_run1")) %>%
    rename(dist_method = metric) %>% 
    group_by(dist_method, pipe) %>% 
    nest() %>% 
    mutate(fit = map(data, ~lm(mean_dist~ norm, data = .)))
```

All significant
```{r}
tech_art_norm_fit %>% mutate(glance_fit = map(fit, broom::glance)) %>% 
    unnest(glance_fit) %>% 
    select(-data, -fit) %>% 
    gather(metric, value, -dist_method, -pipe) %>%
    spread(dist_method, value) %>%
    filter(metric %in% c("adj.r.squared","AIC", "p.value")) %>% 
    knitr::kable(digits = 3)
```
```{r}
summary(tech_art_norm_fit$fit[[1]])
```

```{r}
norm_test_df <- tech_art_norm_fit %>% 
    mutate(fit_tidy = map(fit, broom::tidy)) %>% 
    unnest(fit_tidy) %>% 
    mutate(adj.p.value = p.adjust(p.value, method = "bon"))
```

```{r}
norm_test_df %>% 
    mutate(adj.p.value = if_else(adj.p.value > 0.1, 0.1, adj.p.value)) %>%
    mutate(pos_neg = if_else(estimate > 0, "red", "white")) %>% 
    mutate(estimate = if_else(adj.p.value < 0.1, estimate, NaN)) %>% 
    ggplot() + 
    geom_raster(aes(x = term, y = pipe, fill = adj.p.value)) + 
    geom_text(aes(x = term, y = pipe, label = round(estimate,2), color = pos_neg)) + 
    facet_grid(dist_method~.) + 
    theme_bw() + 
    theme(axis.text.x = element_text(angle = -45, hjust = 0))
```

__NOTES__  
_methods_  
- Fit individual linear models to D~norm for each pipeline and diversity metric.  
- Used F-test to compare values to raw count values (model intercept)  

_results_  
* CSS no significant difference for Bray, Weighted UniFrac significant decrease for deblur and open ref but higher for DADA  
* No difference for UQ  
* TSS lower for Bray excluding deblur and dada, lower for Open ref and bray for Wunifrac  
* Rarefaction  
    - Closed ref rare did not improve repeatability  
    - Rareq15 no differences or lowered repeat for all butn Jaccard mothur and Denovo Unifrac 
    - Norm 2000   
        - improved repeat for all but closed ref (unweighted), not significant for deblur  
        - weighted imporved for mothur bray and deblur, open ref wunif, not significantly difference for rest  
    - Norm 5000 similar results to 2000 but fewer significant.  

_interpretation_  
* Norm TMM and RLE best for weighted metrics, followed by TSS   
* Mixed results for rarefaction  
* rare2000 best for unweighted  

_additional analysis_   
- mixed effects model with replicate as random effect   
- check for consistent results with NIST 2 and JHU 2  

### Diagnostic plots
```{r}
tech_art_norm_fit %>% 
    mutate(plot_lab = paste(dist_method, pipe)) %>% 
    mutate(diag_plots = map2(fit, plot_lab, ~{autoplot(.x) + ggtitle(.y)})) %>% 
    {print(.$diag_plots)}
```

